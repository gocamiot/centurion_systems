import subprocess
import time, re, os
import requests
from django.shortcuts import render, redirect, get_object_or_404
from django.core.paginator import Paginator, EmptyPage, PageNotAnInteger
from apps.tables.utils import device_filter
from django.conf import settings
from apps.tables.models import ( Favorite, UserFilter, 
	PageItems, HideShowFilter, ModelChoices, 
	DateRangeFilter, TableDropdownItem, 
	TableDropdownSubItem, DependentDropdown
)
from home.models import JoinModel
from apps.common.models import Sidebar
from django.http import JsonResponse
from datetime import datetime
from django.utils import timezone
from loader.models import InstantUpload, TypeChoices, hash_json, SAPApi, DynamicQuery, json_to_vector, is_pgvector_enabled
from loader.views import update_model_choices_file, run_migrations, model_exists
from django.contrib.contenttypes.models import ContentType
from django.db.models import Q
from django.apps import apps
from django.db import models, connections
from django.contrib import messages
from celery import shared_task, chain
from django.urls import reverse
from django.db import connection, transaction
from apps.view_builder.views import update_template_tags
from django.contrib.auth.models import Group
from loader.admin import get_selected_models
from django.conf import settings

try:
    from pgvector.django import VectorField
    PGVECTOR_AVAILABLE = True
except ImportError:
    PGVECTOR_AVAILABLE = False

def get_user_id(request):
	if request.user.is_authenticated:
		return request.user.pk
	else:
		return -1


# def get_device_data(request):
# 	snapshot_filter = {}
# 	content_type = ContentType.objects.get(model='controladuser')
# 	snapshots = InstantUpload.objects.filter(content_type=content_type).order_by('-created_at')
# 	latest_snapshot = snapshots.latest('created_at')
# 	snapshot_filter['loader_instance'] = latest_snapshot.pk
# 	devices = ControlADUser.objects.filter(**snapshot_filter).filter(
# 		Q(AD_Enabled_Status='TRUE') | Q(AD_Enabled_Status='True')
# 	)
# 	chart_data = {}
# 	consolidated_chart_data = []
# 	for device in devices:
# 		timestamp = device.AD_Account_Created
# 		dt_object = datetime.utcfromtimestamp(timestamp)
# 		month = dt_object.month
# 		if month not in chart_data:
# 			chart_data[month] = {}

# 		if 'count' not in chart_data[month]:
# 			chart_data[month]['count'] = 1
# 			chart_data[month]['date'] = device.AD_Account_Created
# 		else:
# 			chart_data[month]['count'] += 1


# 	for month, data in chart_data.items():
# 		consolidated_chart_data.append({
# 			'x': month,
# 			'y': data['count'],
# 			'date': data['date'],
# 			'deviceType': 'All',
# 		})

# 	return JsonResponse({'consolidatedChartData': consolidated_chart_data}, safe=False)


def index(request):
	# db_field_names = [field.name for field in Devices._meta.get_fields() if not field.is_relation if not '_Q_' in field.name]
	# field_names = []
	# for field_name in db_field_names:
	# 	fields, created = HideShowFilter.objects.get_or_create(key=field_name, userID=get_user_id(request), parent=ModelChoices.DEVICES)
	# 	field_names.append(fields)

	# page_items = PageItems.objects.filter(userID=get_user_id(request), parent=ModelChoices.DEVICES).last()
	# items = 25
	# if page_items:
	# 	items = page_items.items_per_page

	# filter_string = {}
	# Pre-FILTERS
	#filter_string['MatchedTitle__icontains'] = 'hdx'

	# filter_instance = UserFilter.objects.filter(userID=get_user_id(request), parent=ModelChoices.DEVICES)
	# for filter_data in filter_instance:
	# 	filter_string[f'{filter_data.key}__icontains'] = filter_data.value
	

	# date_filter_instance = DateRangeFilter.objects.filter(userID=get_user_id(request), parent=ModelChoices.DEVICES)
	# for date_filter in date_filter_instance:
	# 	from_date = datetime.strptime(date_filter.from_date.strftime('%Y-%m-%d'), "%Y-%m-%d").timestamp()
	# 	to_date = datetime.strptime((date_filter.to_date + timezone.timedelta(days=1)).strftime('%Y-%m-%d'), "%Y-%m-%d").timestamp()
	# 	filter_string[f'{date_filter.key}__range'] = [from_date, to_date]


	# order_by = request.GET.get('order_by', 'ID')
	# queryset = Devices.objects.filter(**filter_string).order_by(order_by)

	# device_list = device_filter(request, queryset, db_field_names)
	# form = DeviceForm()

	# page = request.GET.get('page', 1)
	# paginator = Paginator(device_list, items)

	# favorites = Favorite.objects.filter(user=request.user)

	
	# try:
	# 	devices = paginator.page(page)
	# except PageNotAnInteger:
	# 	return redirect('list_of_devices')
	# except EmptyPage:
	# 	return redirect('list_of_devices') 

	context = {
		'segment'  : 'dashboard',
		#'form'     : form,
		#'devices' : devices,
		#'db_field_names': db_field_names,
		#'favorites': favorites
	}

	return render(request, "dashboard/index.html", context)


def starter(request):

	context = {}
	return render(request, "pages/grc_starter.html", context)


# Layout
def stacked(request):
	context = {
		'segment': 'stacked',
		'parent': 'layouts'
	}
	return render(request, 'pages/layouts/stacked.html', context)

def sidebar(request):
	context = {
		'segment': 'sidebar',
		'parent': 'layouts'
	}
	return render(request, 'pages/layouts/sidebar.html', context)


# CRUD
def products(request):
	context = {
		'segment': 'products',
		'parent': 'crud'
	}
	return render(request, 'pages/CRUD/products.html', context)

def users(request):
	context = {
		'segment': 'users',
		'parent': 'crud'
	}
	return render(request, 'pages/CRUD/users.html', context)


# Pages
def pricing(request):
	return render(request, 'pages/pricing.html')

def maintenance(request):
	return render(request, 'pages/maintenance.html')

def error_404(request):
	return render(request, 'pages/404.html')

def error_500(request):
	return render(request, 'pages/500.html')

def settings_view(request):
	context = {
		'segment': 'settings',
	}
	return render(request, 'pages/settings.html', context)


# Playground
def stacked_playground(request):
	return render(request, 'pages/playground/stacked.html')


def sidebar_playground(request):
	context = {
		'segment': 'sidebar_playground',
		'parent': 'playground'
	}
	return render(request, 'pages/playground/sidebar.html', context)


# i18n
def i18n_view(request):
	context = {
		'segment': 'i18n'
	}
	return render(request, 'pages/i18n.html', context)



# table dropdown

def create_table_dropdown(request):
	dropdown_qs = TableDropdownItem.objects.all()
	if request.method == 'POST':
		if item := request.POST.get('item'):
			dropdown, created = TableDropdownItem.objects.get_or_create(item=item)
			if created:
				dropdown.created_at = timezone.now()
				dropdown.updated_at = timezone.now()
				dropdown.save()
			
			return redirect(request.META.get('HTTP_REFERER'))
	

	page = request.GET.get('page', 1)
	items = 15
	paginator = Paginator(dropdown_qs, items)
	
	try:
		dropdowns = paginator.page(page)
	except PageNotAnInteger:
		return redirect('create_table_dropdown')
	except EmptyPage:
		return redirect('create_table_dropdown') 

	context = {
		'dropdowns': dropdowns,
		'parent': 'admin',
		'segment': 'table_dropdown'
	}
	return render(request, 'pages/dropdowns/create-items.html', context)


def update_table_dropdown(request, id):
	dropdown = TableDropdownItem.objects.get(pk=id)
	if request.method == 'POST':
		if item := request.POST.get('item'):
			dropdown.item = item
			dropdown.updated_at = timezone.now()
			dropdown.save()

		return redirect(request.META.get('HTTP_REFERER'))
	

def delete_table_dropdown(request, id):
	dropdown = TableDropdownItem.objects.get(pk=id)
	dropdown.delete()

	return redirect(request.META.get('HTTP_REFERER'))


def create_sub_items(request, id):
	item = TableDropdownItem.objects.get(pk=id)
	sub_items = TableDropdownSubItem.objects.filter(item=item)
	if request.method == 'POST':
		if sub_item := request.POST.get('subitem'):
			TableDropdownSubItem.objects.create(
				item=item, 
				subitem=sub_item,
				created_at=timezone.now(),
				updated_at=timezone.now()
			)

		return redirect(request.META.get('HTTP_REFERER'))
	
	context = {
		'item': item,
		'sub_items': sub_items
	}
	return render(request, 'pages/dropdowns/create-sub-items.html', context)


def update_sub_item(request, id):
	sub_item = TableDropdownSubItem.objects.get(pk=id)
	if request.method == 'POST':
		if subitem := request.POST.get('subitem'):
			sub_item.subitem = subitem
			sub_item.updated_at = timezone.now()
			sub_item.save()

		return redirect(request.META.get('HTTP_REFERER'))
	
def delete_sub_item(request, id):
	sub_item = TableDropdownSubItem.objects.get(pk=id)
	sub_item.delete()
	return redirect(request.META.get('HTTP_REFERER'))


# Dependent Dropdown

def create_dependent_dropdown(request):
	dropdown_qs = DependentDropdown.objects.filter(featured=True)
	if request.method == 'POST':
		if title := request.POST.get('title'):
			dropdown, created = DependentDropdown.objects.get_or_create(title=title)
			if created:
				dropdown.featured = True
				dropdown.created_at = timezone.now()
				dropdown.updated_at = timezone.now()
				dropdown.save()
			
			return redirect(request.META.get('HTTP_REFERER'))

	page = request.GET.get('page', 1)
	items = 15
	paginator = Paginator(dropdown_qs, items)
	
	try:
		dropdowns = paginator.page(page)
	except PageNotAnInteger:
		return redirect('create_dependent_dropdown')
	except EmptyPage:
		return redirect('create_dependent_dropdown') 

	context = {
		'dropdowns': dropdowns,
		'items': dropdown_qs,
		'parent': 'admin',
		'segment': 'dependent_dropdown'
	}
	return render(request, 'pages/dependent/create-items.html', context)

def update_dependent_dropdown(request, id):
	dropdown = DependentDropdown.objects.get(pk=id)
	if request.method == 'POST':
		if title := request.POST.get('title'):
			if not DependentDropdown.objects.filter(parent=dropdown, title=title).exists():
				dropdown.title = title
				dropdown.updated_at = timezone.now()
				dropdown.save()
			else:
				messages.error(request, 'Dropdown with this title already exists')

		return redirect(request.META.get('HTTP_REFERER'))


def delete_dependent_dropdown(request, id):
	dropdown = DependentDropdown.objects.get(pk=id)
	dropdown.delete()

	return redirect(request.META.get('HTTP_REFERER'))


def create_dependent_sub_items(request, id):
	item = DependentDropdown.objects.get(pk=id)
	sub_items = DependentDropdown.objects.filter(parent=item, featured=False)
	if request.method == 'POST':
		if title := request.POST.get('title'):
			dropdown, created = DependentDropdown.objects.get_or_create(title=title, parent=item)
			if created:
				dropdown.created_at = timezone.now()
				dropdown.updated_at = timezone.now()
				dropdown.save()
		return redirect(request.META.get('HTTP_REFERER'))
	
	context = {
		'item': item,
		'sub_items': sub_items
	}
	return render(request, 'pages/dependent/create-sub-items.html', context)



# Join







from django.conf import settings
from celery.result import AsyncResult



def view_function(request_data, selected_model):
    parent_name = get_object_or_404(Sidebar, id=request_data.get('parent_name'))
    if selected_model:
        app_name, model_name = selected_model.split('.')
        model_class = apps.get_model(app_name, model_name)

    update_template_tags(model_class)

    fav_name = request_data.get('fav_name')
    view_name = fav_name.replace(' ', '_').lower()
    view_name = re.sub(r'[^a-zA-Z0-9_]', '', view_name)

    model_choice = '_'.join(re.findall(r'[A-Z]+(?![a-z])|[A-Z][a-z]*|\d+', model_name)).upper()
    db_table = model_class._meta.db_table.split('_', 1)[-1]

    current_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.join(settings.BASE_DIR)
    template_path = os.path.join(base_dir, 'apps', 'view_builder', 'control', 'base_control_view.txt')
    with open(template_path, 'r') as file:
        template_content = file.read()

    
    updated_content = template_content

    readonly_fields = request_data.get('ro_fields') or []
    readonly_fields = list(set(readonly_fields + ['ID', 'loader_instance', 'json_data', 'hash_data']))
    if readonly_fields:
        readonly_fields_str = ', '.join([f"'{field}'" for field in readonly_fields])
        updated_content = updated_content.replace(
            'read_only_fields = ()',
            f"read_only_fields = ({readonly_fields_str},)"
        )

    pre_columns = request_data.get('precol_fields') or []
    pre_columns = list(set(pre_columns + ['ID', 'loader_instance', 'json_data', 'hash_data']))
    if pre_columns:
        pre_columns_str = ', '.join([f"'{field}'" for field in pre_columns])
        updated_content = updated_content.replace(
            'pre_column = ()',
            f"pre_column = ({pre_columns_str},)"
        )

    compulsory_fields = request_data.get('compulsory_fields') or []
    if compulsory_fields:
        compulsory_fields_str = ', '.join([f"'{field}'" for field in compulsory_fields])
        updated_content = updated_content.replace(
            'compulsory_fields = ()',
            f"compulsory_fields = ({compulsory_fields_str},)"
        )

    parent = parent_name.segment if parent_name.segment else parent_name.name

    updated_content = updated_content.replace('VIEW_NAME', view_name).replace('MODEL_CHOICE', model_choice).replace('MODEL_NAME', model_name).replace('DB_TABLE', db_table).replace('PARENT_NAME', parent).replace('FAV_NAME', fav_name)


    view_dir_path = os.path.join(settings.VIEW_PATH, view_name)
    if not os.path.exists(view_dir_path):
        os.makedirs(view_dir_path)

    output_file_path = os.path.join(view_dir_path, f'{view_name}_views.py')
    with open(output_file_path, 'w') as output_file:
        output_file.write(updated_content)


def url_function(request_data):
    fav_name = request_data.get('fav_name')
    view_name = fav_name.replace(' ', '_').lower()
    view_name = re.sub(r'[^a-zA-Z0-9_]', '', view_name)

    base_dir = os.path.join(settings.BASE_DIR)
    template_path = os.path.join(base_dir, 'apps', 'view_builder', 'control', 'base_control_urls.txt')

    with open(template_path, 'r') as file:
        template_content = file.read()

    updated_content = template_content.replace('VIEW_NAME', view_name).replace('URL_NAME', view_name.replace('_', '-'))

    url_dir_path = os.path.join(settings.VIEW_PATH, view_name)
    if not os.path.exists(url_dir_path):
        os.makedirs(url_dir_path)

    output_file_path = os.path.join(url_dir_path, f'{view_name}_urls.py')
    with open(output_file_path, 'w') as output_file:
        output_file.write(updated_content)
    

    urls_path = os.path.join(getattr(settings, 'URL_PATH'), 'urls.py')
    include_statement = f"\n    path('tables/', include('apps.tables.{view_name}.{view_name}_urls')),\n"

    with open(urls_path, 'r') as urls_file:
        existing_urls_content = urls_file.read()

    if include_statement.strip() not in existing_urls_content:
        if 'urlpatterns = [' in existing_urls_content:
            last_closing_bracket_pos = existing_urls_content.rfind(']')
            
            new_urls_content = (
                existing_urls_content[:last_closing_bracket_pos]
                + f"{include_statement}"
                + existing_urls_content[last_closing_bracket_pos:]
            )

            with open(urls_path, 'w') as urls_file:
                urls_file.write(new_urls_content)


def template_function(request_data, selected_model):
    if selected_model:
        app_name, model_name = selected_model.split('.')
        model_class = apps.get_model(app_name, model_name)

    fav_name = request_data.get('fav_name')
    view_name = fav_name.replace(' ', '_').lower()
    view_name = re.sub(r'[^a-zA-Z0-9_]', '', view_name)

    model_choice = '_'.join(re.findall(r'[A-Z]+(?![a-z])|[A-Z][a-z]*|\d+', model_name)).upper()

    base_dir = os.path.join(settings.BASE_DIR)
    template_path = os.path.join(base_dir, 'apps', 'view_builder', 'control', 'base_control_template.txt')

    with open(template_path, 'r') as file:
        template_content = file.read()

    updated_content = template_content.replace('VIEW_NAME', view_name).replace('MODEL_CHOICE', model_choice).replace('MODEL_NAME', model_name).replace('FAV_NAME', fav_name).replace('URL_NAME', view_name.replace('_', '-'))

    output_file_path = os.path.join(settings.TEMPLATE_PATH, f'{view_name}.html')
    with open(output_file_path, 'w') as output_file:
        output_file.write(updated_content)


def table_exists(table_name):
    with connection.cursor() as cursor:
        cursor.execute("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_name = %s
            );
        """, [table_name])
        return cursor.fetchone()[0]

@shared_task
def run_local_migrations(result):
    try:
        print("Starting migrations...")
        subprocess.run('python manage.py makemigrations --noinput', shell=True, check=True)
        print("Makemigrations completed.")
        subprocess.run('python manage.py migrate --noinput', shell=True, check=True)
        print("Migrate completed.")
        result['status'] = 'completed'
        return result
    except subprocess.CalledProcessError as e:
        result['status'] = 'error'
        result['message'] = str(e)
        raise

@shared_task
def generate_views_urls_templates(result):
    try:
        if result['status'] == 'completed':
            request_data = result['request_data']
            selected_model = f"tables.{JoinModel.objects.get(id=result['join_model_id']).model_name}"
            view_function(request_data, selected_model)
            url_function(request_data)
            template_function(request_data, selected_model)
            print(f"Views, URLs, and templates for {selected_model} created successfully!")
        else:
            print("Failed to generate views, URLs, and templates due to model generation error.")
        return result
    except Exception as e:
        result['status'] = 'error'
        result['message'] = str(e)

@shared_task
def generate_model_and_migrate(join_model_id, request_data):
    try:
        join_model = JoinModel.objects.get(id=join_model_id)
        models_file_path = settings.MODELS_FILE_PATH

        model_name = join_model.model_name
        base_model_class = join_model.base_model.model_class()

        delta_model_class = None
        delta_fields = []
        delta_field_definitions = ""
        
        has_delta_model = bool(join_model.delta_model)
        if has_delta_model:
            delta_model_class = join_model.delta_model.model_class()
            delta_fields = join_model.delta_fields.split(',') if join_model.delta_fields else []

        base_fields = join_model.base_fields.split(',') if join_model.base_fields else []

        special_categories = [
            "date_fields_to_convert", 
            "integer_fields", 
            "float_fields", 
            "encrypted_fields", 
            "unix_dates", 
            "ad_unix_dates"
        ]
        special_field_dict = {category: set() for category in special_categories}

        def get_field_definition(model_class, field_name, prefix=""):
            field = model_class._meta.get_field(field_name)
            field_type = field.get_internal_type()

            if field_name.startswith('_') or field_name.lower() == 'count':
                sanitized_name = field_name.lstrip('_')
                field_name_with_prefix = f"{prefix}{sanitized_name}_original"
            else:
                field_name_with_prefix = f"{prefix}{field_name}"

            for category in special_categories:
                if hasattr(model_class, category) and field_name in getattr(model_class, category):
                    special_field_dict[category].add(field_name_with_prefix)

            if isinstance(field, models.ForeignKey):
                return f"    {field_name_with_prefix} = models.ForeignKey('{field.related_model._meta.label}', on_delete=models.CASCADE)"
            elif isinstance(field, models.BigIntegerField):
                return f"    {field_name_with_prefix} = models.BigIntegerField(null=True, blank=True)"
            elif isinstance(field, models.IntegerField):
                return f"    {field_name_with_prefix} = models.IntegerField(null=True, blank=True)"
            elif isinstance(field, models.BooleanField):
                return f"    {field_name_with_prefix} = models.BooleanField(null=True, blank=True)"
            elif isinstance(field, models.DateTimeField):
                return f"    {field_name_with_prefix} = models.DateTimeField(null=True, blank=True)"
            elif isinstance(field, models.DateField):
                return f"    {field_name_with_prefix} = models.DateField(null=True, blank=True)"
            elif isinstance(field, models.FloatField):
                return f"    {field_name_with_prefix} = models.FloatField(null=True, blank=True)"
            elif isinstance(field, models.TextField):
                return f"    {field_name_with_prefix} = models.TextField(null=True, blank=True)"
            else:
                return f"    {field_name_with_prefix} = models.CharField(max_length=255, null=True, blank=True)"

        base_field_definitions = "\n".join([get_field_definition(base_model_class, field) for field in base_fields])
        if has_delta_model:
            delta_field_definitions = "\n".join([get_field_definition(delta_model_class, field, prefix="d_") for field in delta_fields])

        special_field_definitions = "\n".join([
            f"    {category} = {list(special_field_dict[category])}" for category in special_categories
        ])

        comparison_fields = ""
        if has_delta_model:
            comparison_fields = """
    both = models.IntegerField(null=True, blank=True)
    base = models.IntegerField(null=True, blank=True)
    delta = models.IntegerField(null=True, blank=True)
"""

        model_code = f"""\n
class {model_name}(models.Model):
    ID = models.AutoField(primary_key=True)
{base_field_definitions}
{delta_field_definitions if has_delta_model else ''}
{comparison_fields}
    loader_instance = models.IntegerField(null=True, blank=True)
    json_data = models.JSONField(null=True, blank=True)
    hash_data = models.TextField(null=True, blank=True)\n
    join_model_instance = {join_model.pk}  # Do not change this value
{special_field_definitions}
"""

        with open(models_file_path, "r") as f:
            original_content = f.read()

        with transaction.atomic():
            with open(models_file_path, "a") as f:
                f.write(model_code)
            update_model_choices_file(model_name)
            print("Model code added successfully!")

            task_chain = chain(
                run_local_migrations.s({'status': 'pending', 'join_model_id': join_model.id, 'request_data': request_data}),
                generate_views_urls_templates.s()
            )
            result = task_chain.apply_async()
            task_id = result.id

            return {
                'status': 'pending',
                'task_id': task_id,
                'message': 'Model creation in progress. Please wait...'
            }

    except Exception as e:
        print(f"Error encountered: {e}")
        with open(models_file_path, "w") as f:
            f.write(original_content)
        print(f"Failed to add Model {model_name}!")
        return {
            'status': 'error',
            'message': str(e)
        }

def join_models(request):
    if request.method == 'POST':
        join_model_name = request.POST.get('join_model_name')
        base_model_name = request.POST.get('base_model') 
        delta_model_name = request.POST.get('delta_model', '')
        base_fields = request.POST.getlist('base_model_fields', [])
        delta_fields = request.POST.getlist('delta_model_fields', [])
        base_f_key_fields = request.POST.getlist('base_foreign_key_fields', [])
        delta_f_key_fields = request.POST.getlist('delta_foreign_key_fields', [])
        join_method = request.POST.get('join_method')
        similarity_threshold = request.POST.get('similarity_threshold')
        split_char = request.POST.get('split_char')
        word_position = request.POST.get('word_position')
        split_space = request.POST.get('split_space') == 'on'

        jaro_winkler = request.POST.get('jaro_winkler') == 'on'
        levenshtein = request.POST.get('levenshtein') == 'on'
        token_set_ratio = request.POST.get('token_set_ratio') == 'on'
        phonetic = request.POST.get('phonetic') == 'on'
        fuzzy_wuzzy = request.POST.get('fuzzy_wuzzy') == 'on'

        py_function = request.POST.get('py_function')
        c_percent = request.POST.get('c_percent')

        left_split_char = request.POST.get('left_split_char')
        email_pre_char = request.POST.get('pre_char_email')
        email_post_char = request.POST.get('post_char_email')

        if table_exists(f'tables_{join_model_name.lower()}'):
            return JsonResponse({
                'status': 'error',
                'message': f'Table for model "{join_model_name}" already exists in the database.'
            })

        if JoinModel.objects.filter(model_name=join_model_name).exists():
            return JsonResponse({
                'status': 'error',
                'message': f'Model with name "{join_model_name}" already exists.'
            })

        base_model_parts = base_model_name.split('.')
        base_model_content_type = ContentType.objects.get(app_label=base_model_parts[0], model=base_model_parts[1].lower().replace('_', ''))
        
        delta_model_content_type = None
        if delta_model_name:
            delta_model_parts = delta_model_name.split('.')
            delta_model_content_type = ContentType.objects.get(app_label=delta_model_parts[0], model=delta_model_parts[1].lower().replace('_', ''))

        try:
            join_model = JoinModel.objects.create(
                model_name=join_model_name,
                base_model=base_model_content_type,
                delta_model=delta_model_content_type,
                base_fields=",".join(base_fields) if base_fields else "",
                delta_fields=",".join(delta_fields) if delta_fields else "",
                base_f_key_fields=",".join(base_f_key_fields) if base_f_key_fields else "",
                delta_f_key_fields=",".join(delta_f_key_fields) if delta_f_key_fields else "",
                join_method=join_method,
                similarity_threshold=similarity_threshold,
                split_character=split_char,
                split_by_space=split_space,
                match_word_position=word_position,
                jaro_winkler=jaro_winkler,
                levenshtein=levenshtein,
                token_set_ratio=token_set_ratio,
                phonetic=phonetic,
                fuzzy_wuzzy=fuzzy_wuzzy,
                python_function=py_function,
                c_similarity_threshold=c_percent,
                left_split_char=left_split_char,
                email_pre_char=email_pre_char,
                email_post_char=email_post_char
            )
            result = generate_model_and_migrate.apply_async(args=[join_model.id, request.POST.dict()])
            task_id = result.id

            fav_name = request.POST.get('fav_name')
            view_name = fav_name.replace(' ', '_').lower()
            view_name = re.sub(r'[^a-zA-Z0-9_]', '', view_name)

            sidebar_group = get_object_or_404(Group, name=request.POST.get('group_name'))
            sidebar_parent = get_object_or_404(Sidebar, id=request.POST.get('parent_name'))
            Sidebar.objects.create(
                group=sidebar_group,
                parent=sidebar_parent,
                name=fav_name,
                segment=view_name,
                url_name=view_name
            )

            return JsonResponse({
                'status': 'pending',
                'task_id': task_id,
                'view_name': view_name,
                'message': 'Model creation in progress. Please wait...'             
            })

        except Exception as e:
            return redirect(request.META.get('HTTP_REFERER'))

    loader_model_apps = ['tables']
    selected_models = get_selected_models()
    model_names = {}
    for app_name in loader_model_apps:
        app_config = apps.get_app_config(app_name)
        models = [model.__name__ for model in app_config.get_models()]

        filtered_models = sorted([model for model in models if f"{app_name}.{model}" not in selected_models])
        
        if filtered_models:
            model_names[app_name] = filtered_models

    sidebars = Sidebar.objects.filter(is_active=True, parent__isnull=True).values('id', 'name')
    groups = Group.objects.all()

    context = {
        'model_names': model_names,
        'sidebars': sidebars,
        'groups': groups,
        'segment': 'join_models',
        'parent' : 'home'
    }

    return render(request, 'pages/join_models.html', context)


def check_task_status(request):
    task_id = request.GET.get("task_id")

    if not task_id:
        return JsonResponse({"status": "error", "message": "Task ID is missing"})

    task_result = AsyncResult(task_id)

    if task_result.state == "SUCCESS":
        return JsonResponse({"status": "completed", "redirect_url": "/"})
    elif task_result.state in ["PENDING", "STARTED"]:
        return JsonResponse({"status": "pending"})
    elif task_result.state == "FAILURE":
        return JsonResponse({"status": "error", "message": str(task_result.result)})
    else:
        return JsonResponse({"status": "error", "message": "Unknown task state"})
	

def check_model_exists(request):
    model_name = request.GET.get('model_name')
    loader_model_apps = settings.LOADER_MODEL_APPS
    
    for app_name in loader_model_apps:
        if model_name in [model.__name__ for model in apps.get_app_config(app_name).get_models()]:
            return JsonResponse({'exists': True})
    
    return JsonResponse({'exists': False})


from home.models import IPE, PyFunction
from urllib.parse import urlparse
from rapidfuzz.fuzz import ratio, token_set_ratio, WRatio
from fuzzywuzzy import fuzz
import jellyfish


def get_original_field_name(join_field_name):
    field_name = join_field_name
    if field_name.startswith('d_'):
        field_name = field_name[2:]

    if field_name.lower() in ['id_original', 'count_original']:
        return field_name

    if field_name.endswith('_original'):
        field_name = field_name[:-9]
        field_name = f"_{field_name}"

    return field_name

def load_data_to_join_model(request, model_name, pk):
    if request.method != 'POST':
        return redirect(request.META.get('HTTP_REFERER'))

    # --- POST parameters ---
    method = request.POST.get('method')
    percent = request.POST.get('percent')

    c_full_word_match = request.POST.get('c_full_word_match') == 'on'
    py_function = request.POST.get('py_function')
    c_percent = request.POST.get('c_percent')
    fields_to_pass = request.POST.getlist('fields_to_pass')

    split_char = request.POST.get('split_char')
    word_position = request.POST.get('word_position')
    split_space = request.POST.get('split_space') == 'on'
    s_full_word_match = request.POST.get('s_full_word_match') == 'on'

    pre_char = request.POST.get('pre_char')
    post_char = request.POST.get('post_char')
    pp_full_word_match = request.POST.get('pp_full_word_match') == 'on'

    jaro_winkler = request.POST.get('jaro_winkler')
    levenshtein = request.POST.get('levenshtein')
    v_token_set_ratio = request.POST.get('token_set_ratio')
    phonetic = request.POST.get('phonetic')
    fuzzy_wuzzy = request.POST.get('fuzzy_wuzzy')

    left_split_char = request.POST.get('left_split_char')
    email_pre_char = request.POST.get('email_pre_char')
    email_post_char = request.POST.get('email_post_char')

    join_model = JoinModel.objects.get(pk=pk)
    model = apps.get_model('tables', model_name)
    vector_model_name = getattr(model, 'vector_model', 'BAAI/bge-large-en-v1.5')

    loader_instance = InstantUpload.objects.create(
        type=TypeChoices.FILE,
        content_type=ContentType.objects.get_for_model(model)
    )

    base_model_class = join_model.base_model.model_class()
    has_delta_model = bool(join_model.delta_model)
    delta_model_class = join_model.delta_model.model_class() if has_delta_model else None

    max_base_loader_instance = base_model_class.objects.aggregate(models.Max('loader_instance'))['loader_instance__max']
    base_records = list(base_model_class.objects.filter(loader_instance=max_base_loader_instance))

    delta_records = []
    if has_delta_model:
        max_delta_loader_instance = delta_model_class.objects.aggregate(models.Max('loader_instance'))['loader_instance__max']
        delta_records = list(delta_model_class.objects.filter(loader_instance=max_delta_loader_instance))

    base_field = join_model.base_f_key_fields
    delta_f_fields = join_model.delta_f_key_fields.split(',') if join_model.delta_f_key_fields and has_delta_model else []

    base_fields = join_model.base_fields.split(',') if join_model.base_fields else []
    delta_fields = join_model.delta_fields.split(',') if join_model.delta_fields and has_delta_model else []

    def safe_field_name(name):
        if name.startswith('_') or name.lower() == 'count':
            sanitized = name.lstrip('_')
            return f"{sanitized}_original"
        return name

    base_field = safe_field_name(base_field)
    delta_f_fields = [safe_field_name(f) for f in delta_f_fields]
    base_fields = [safe_field_name(f) for f in base_fields]
    delta_fields = [safe_field_name(f) for f in delta_fields]
    fields_to_pass = [safe_field_name(f) for f in fields_to_pass]

    adjoin_instances = []

    # --- Preprocess delta mapping ---
    delta_values_set = set()
    delta_mapping = {}
    if has_delta_model:
        for delta in delta_records:
            delta_values = [str(getattr(delta, field, None)) for field in delta_f_fields if getattr(delta, field, None) is not None]
            normalized_delta_value = "".join(delta_values).replace(" ", "").lower()
            delta_values_set.add(normalized_delta_value)
            delta_mapping[normalized_delta_value] = delta

    # --- Iterate base records ---
    for base_record in base_records:
        base_value = str(getattr(base_record, base_field, None)) if base_record else None
        normalized_base_value = base_value.replace(" ", "").lower() if base_value else None
        raw_base_value = base_value if base_value else None

        matched_delta_record = []

        if has_delta_model and normalized_base_value:

            # ---------- contains method ----------
            if method == 'contains':
                for delta_value in delta_values_set:
                    delta_combined = "".join(
                        [str(getattr(delta_mapping[delta_value], f, "")).strip() for f in delta_f_fields]
                    ).replace(" ", "").lower()

                    normalized_base = normalized_base_value.strip()

                    if c_full_word_match:
                        if normalized_base == delta_combined:
                            matched_delta_record.append(delta_mapping[delta_value])
                    else:
                        if normalized_base in delta_combined:
                            matched_delta_record.append(delta_mapping[delta_value])

            # ---------- winkler_levenshtein method ----------
            elif method == 'winkler_levenshtein':
                required_methods = []
                if jaro_winkler: required_methods.append('jaro_winkler')
                if levenshtein: required_methods.append('levenshtein')
                if v_token_set_ratio: required_methods.append('token_set_ratio')
                if phonetic: required_methods.append('phonetic')
                if fuzzy_wuzzy: required_methods.append('fuzzy_wuzzy')
                if not required_methods: required_methods = ['jaro_winkler','levenshtein','token_set_ratio']

                for delta_value in delta_values_set:
                    meets_threshold = True
                    scores = {}
                    if 'jaro_winkler' in required_methods:
                        scores['jaro_winkler'] = WRatio(normalized_base_value, delta_value)
                        if scores['jaro_winkler'] < int(percent): meets_threshold=False
                    if 'levenshtein' in required_methods:
                        scores['levenshtein'] = ratio(normalized_base_value, delta_value)
                        if scores['levenshtein'] < int(percent): meets_threshold=False
                    if 'token_set_ratio' in required_methods:
                        scores['token_set_ratio'] = token_set_ratio(normalized_base_value, delta_value)
                        if scores['token_set_ratio'] < int(percent): meets_threshold=False
                    if 'phonetic' in required_methods:
                        soundex_base = jellyfish.soundex(normalized_base_value)
                        soundex_delta = jellyfish.soundex(delta_value)
                        scores['phonetic'] = 100 if soundex_base==soundex_delta else 0
                        if scores['phonetic'] < int(percent): meets_threshold=False
                    if 'fuzzy_wuzzy' in required_methods:
                        scores['fuzzy_wuzzy'] = fuzz.ratio(normalized_base_value, delta_value)
                        if scores['fuzzy_wuzzy'] < int(percent): meets_threshold=False
                    if meets_threshold:
                        matched_delta_record.append(delta_mapping[delta_value])

            # ---------- split method ----------
            elif method == 'split':
                extracted_words = []
                temp_value = raw_base_value
                if split_char:
                    try:
                        position = int(word_position) - 1
                        parts = temp_value.split(split_char)
                        temp_value = parts[position] if 0 <= position < len(parts) else ""
                    except (ValueError, IndexError):
                        temp_value = ""
                if split_space:
                    words = temp_value.split()
                    try:
                        position = int(word_position) - 1
                        extracted_words = words[position:] if position < len(words) else []
                    except (ValueError, IndexError):
                        extracted_words = []

                if extracted_words:
                    normalized_extracted_words = [word.lower() for word in extracted_words]
                    for delta_value in delta_values_set:
                        delta_combined = "".join([str(getattr(delta_mapping[delta_value], f, "")) for f in delta_f_fields]).replace(" ", "").lower()
                        if s_full_word_match:
                            if "".join(normalized_extracted_words) == delta_combined:
                                matched_delta_record.append(delta_mapping[delta_value])
                        else:
                            if all(word in delta_combined for word in normalized_extracted_words):
                                matched_delta_record.append(delta_mapping[delta_value])

            # ---------- pre_post method ----------
            elif method == 'pre_post':
                modified_base_value = normalized_base_value
                if pre_char and pre_char.isdigit():
                    remove_count = int(pre_char)
                    modified_base_value = modified_base_value[remove_count:] if remove_count < len(modified_base_value) else ""
                if post_char and post_char.isdigit():
                    remove_count = int(post_char)
                    modified_base_value = modified_base_value[:-remove_count] if remove_count < len(modified_base_value) else ""

                for delta_value in delta_values_set:
                    delta_combined = "".join([str(getattr(delta_mapping[delta_value], f, "")) for f in delta_f_fields]).replace(" ", "").lower()
                    if pp_full_word_match:
                        if modified_base_value == delta_combined:
                            matched_delta_record.append(delta_mapping[delta_value])
                    else:
                        if modified_base_value in delta_combined:
                            matched_delta_record.append(delta_mapping[delta_value])

            # ---------- custom method ----------
            elif method == 'custom' and py_function:
                local_vars = {}
                py_func = get_object_or_404(PyFunction, id=py_function)
                try:
                    exec(py_func.func, {}, local_vars)
                    func = next((v for v in local_vars.values() if callable(v)), None)
                    if func:
                        for delta_record in delta_records:
                            base_values = [str(getattr(base_record, f, "")) for f in fields_to_pass if not f.startswith('d_')]
                            delta_values = [str(getattr(delta_record, f[2:], "")) for f in fields_to_pass if f.startswith('d_')]
                            try:
                                result = func(*base_values, *delta_values)
                                if result:
                                    matched_delta_record.append(delta_record)
                            except:
                                continue
                except Exception as e:
                    print(f"Error executing custom function: {e}")

            # ---------- email_join method ----------
            elif method == 'email_join':
                temp_value = normalized_base_value.split('@')[0] if normalized_base_value else ""
                if left_split_char:
                    temp_value = temp_value.split(left_split_char)[0]
                if email_pre_char and email_pre_char.isdigit():
                    remove_count = int(email_pre_char)
                    temp_value = temp_value[remove_count:] if remove_count < len(temp_value) else ""
                if email_post_char and email_post_char.isdigit():
                    remove_count = int(email_post_char)
                    temp_value = temp_value[:-remove_count] if remove_count < len(temp_value) else ""
                for delta_value in delta_values_set:
                    delta_combined = "".join([str(getattr(delta_mapping[delta_value], f, "")) for f in delta_f_fields]).replace(" ", "").lower()
                    if temp_value in delta_combined:
                        matched_delta_record.append(delta_mapping[delta_value])

        # --- Build adjoin_data ---
        adjoin_data = {'loader_instance': loader_instance.pk, 'json_data': None, 'hash_data': None}
        if matched_delta_record:
            for rec in matched_delta_record:
                adjoin_copy = adjoin_data.copy()
                adjoin_copy.update({'both':1, 'base':0, 'delta':0})


                for f in base_fields:
                    adjoin_copy[f] = getattr(base_record, f, None)

                for f in delta_fields:
                    join_field_name = f"d_{f}"
                    orig_field_name = get_original_field_name(f)
                    adjoin_copy[join_field_name] = getattr(rec, orig_field_name, None)

                # create json_data and hash for this copy
                db_alias = model._default_manager.db
                database_path = connections[db_alias].settings_dict['NAME']
                database_name = os.path.basename(database_path) if database_path else database_path
                json_data = {
                    'database': database_name,
                    'table': model._meta.db_table,
                    'loader_instance': loader_instance.pk,
                    'snapshot_at': timezone.now().isoformat(),
                    'both': adjoin_copy['both'],
                    'base': adjoin_copy['base'],
                    'delta': adjoin_copy['delta']
                }
                
                # for f in base_fields: json_data[f] = adjoin_copy.get(f)
                # for f in delta_fields: json_data[f"d_{f}"] = adjoin_copy.get(f"d_{f}")
                for f in base_fields:
                    json_data[f] = getattr(base_record, f, None)

                for f in delta_fields:
                    join_field_name = f"d_{f}"
                    orig_field_name = get_original_field_name(f)
                    json_data[join_field_name] = getattr(rec, orig_field_name, None)


                adjoin_copy['json_data'] = json_data
                if is_pgvector_enabled():
                    adjoin_copy['hash_data'] = json_to_vector(json_data)
                else:
                    adjoin_copy['hash_data'] = hash_json(json_data)

                adjoin_instances.append(model(**adjoin_copy))
        else:
            adjoin_data.update({'both':0, 'base':1, 'delta':0})
            for f in base_fields:
                adjoin_data[f] = getattr(base_record, f, None)

            db_alias = model._default_manager.db
            database_path = connections[db_alias].settings_dict['NAME']
            database_name = os.path.basename(database_path) if database_path else database_path
            json_data = {
                'database': database_name,
                'table': model._meta.db_table,
                'loader_instance': loader_instance.pk,
                'snapshot_at': timezone.now().isoformat(),
                'both': adjoin_data['both'],
                'base': adjoin_data['base'],
                'delta': adjoin_data['delta']
            }
            # for f in base_fields: json_data[f] = adjoin_data.get(f)
            # for f in delta_fields: json_data[f"d_{f}"] = adjoin_data.get(f"d_{f}")

            for f in base_fields:
                json_data[f] = getattr(base_record, f, None)

            for f in delta_fields:
                join_field_name = f"d_{f}"
                json_data[join_field_name] = None

            adjoin_data['json_data'] = json_data
            if is_pgvector_enabled():
                adjoin_data['hash_data'] = json_to_vector(json_data, model_name=vector_model_name)
            else:
                adjoin_data['hash_data'] = hash_json(json_data)

            adjoin_instances.append(model(**adjoin_data))

    # --- Add delta-only rows ---
    if has_delta_model:
        db_alias = model._default_manager.db
        database_path = connections[db_alias].settings_dict['NAME']
        database_name = os.path.basename(database_path) if database_path else database_path

        for delta_record in delta_records:
            adjoin_data = {'loader_instance': loader_instance.pk, 'both':0, 'base':0, 'delta':1, 'json_data':None, 'hash_data':None}

            for f in delta_fields:
                join_field_name = f"d_{f}"
                orig_field_name = get_original_field_name(f)
                adjoin_data[join_field_name] = getattr(delta_record, orig_field_name, None)

            json_data = {
                'database': database_name,
                'table': model._meta.db_table,
                'loader_instance': loader_instance.pk,
                'snapshot_at': timezone.now().isoformat(),
                'both': 0,
                'base': 0,
                'delta': 1
            }

            for f in delta_fields:
                join_field_name = f"d_{f}"
                json_data[join_field_name] = adjoin_data.get(join_field_name)

            adjoin_data['json_data'] = json_data
            if is_pgvector_enabled():
                adjoin_data['hash_data'] = json_to_vector(json_data, model_name=vector_model_name)
            else:
                adjoin_data['hash_data'] = hash_json(json_data)

            adjoin_instances.append(model(**adjoin_data))


    # --- Bulk create ---
    model.objects.bulk_create(adjoin_instances)

    # --- Logging IPE ---
    user = request.user
    timestamp_local = timezone.localtime(timezone.now())
    timestamp_str = timestamp_local.strftime('%Y-%m-%d %H:%M:%S')
    description_parts = [f'Data extraction: {base_model_class.__name__}' + (f' and {delta_model_class.__name__}' if has_delta_model else ''),
                         f'{base_model_class.__name__} rows: {len(base_records)}']
    if has_delta_model: description_parts.append(f'{delta_model_class.__name__} rows: {len(delta_records)}')
    description_parts += [f'New Join table name: {model.__name__}',
                          f'Extracted by: {user.get_full_name() or user.username}',
                          f'Extracted on: {timestamp_str}',
                          f'Rows extracted: {len(adjoin_instances)}',
                          f'Snapshot: {loader_instance.pk}',
                          f'Columns: {", ".join([field.name for field in model._meta.get_fields()])}']
    IPE.objects.create(userID=user,
                       path=urlparse(request.META.get('HTTP_REFERER', request.path)).path,
                       description='\n'.join(description_parts))

    return redirect(request.META.get('HTTP_REFERER'))


def join_view_404(request):
    return render(request, 'pages/join_view_404.html')


# Model builder
import csv
from django.views.decorators.csrf import csrf_exempt
import pandas as pd

def sanitize_header(header):
    sanitized = header.strip()
    sanitized = re.sub(r'[!@#$%^&*()+\-=\[\]{}|;:\'",.<>?/`~]', '', sanitized)
    sanitized = re.sub(r'\s+', '_', sanitized)
    return sanitized

@csrf_exempt
def process_file(request):
    if request.method == "POST" and request.FILES.get('file'):
        uploaded_file = request.FILES['file']
        filename = uploaded_file.name
        extension = os.path.splitext(filename)[1].lower()

        try:
            if extension == '.csv':
                decoded_file = uploaded_file.read().decode('latin-1').splitlines()
                reader = csv.reader(decoded_file)
                headers = next(reader, [])
                sanitized_headers = [sanitize_header(h) for h in headers]

                data = []
                for row in reader:
                    row_dict = dict(zip(sanitized_headers, row))
                    data.append(row_dict)

            elif extension in ['.xlsx', '.xls']:
                df = pd.read_excel(uploaded_file)
                df.columns = [sanitize_header(col) for col in df.columns]
                data = df.to_dict(orient='records')

            else:
                return JsonResponse({'error': 'Unsupported file type'}, status=400)

            return JsonResponse({'headers': list(data[0].keys()) if data else []})

        except Exception as e:
            return JsonResponse({'error': f'Failed to process file: {str(e)}'}, status=500)

    return JsonResponse({'error': 'No file uploaded'}, status=400)

@csrf_exempt
def sap_api_headers(request, id):
    try:
        api_instance = SAPApi.objects.get(id=id)

        headers = {
            'APIKey': api_instance.api_key,
            'Accept': 'application/json',
            'DataServiceVersion': '2.0',
        }

        params = {
            '$top': api_instance.top
        }

        response = requests.get(api_instance.api_url, headers=headers, params=params)

        if response.status_code == 200:
            data = response.json()

            if 'value' in data and isinstance(data['value'], list) and len(data['value']) > 0:
                headers = list(data['value'][0].keys())
                return JsonResponse({'headers': headers})
            else:
                return JsonResponse({'headers': []})

        return JsonResponse({'error': f'HTTP {response.status_code}: {response.text}'}, status=response.status_code)

    except SAPApi.DoesNotExist:
        return JsonResponse({'error': 'SAP API config not found'}, status=404)

    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)
    

@csrf_exempt
def sql_query_headers(request, id):
    try:
        query = DynamicQuery.objects.get(id=id)

        with connection.cursor() as cursor:
            cursor.execute(query.query)
            column_names = [col[0] for col in cursor.description] if cursor.description else []
            exclude_fields = {'id', 'ID', 'loader_instance'}
            filtered_headers = [col for col in column_names if col not in exclude_fields]

        return JsonResponse({'headers': filtered_headers})

    except DynamicQuery.DoesNotExist:
        return JsonResponse({'error': 'DynamicQuery config not found'}, status=404)

    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)


@csrf_exempt
def sap_api_data(request, id):
    try:
        api_instance = SAPApi.objects.get(id=id)

        headers = {
            'APIKey': api_instance.api_key,
            'Accept': 'application/json',
            'DataServiceVersion': '2.0',
        }

        params = {
            '$top': 100 
        }

        response = requests.get(api_instance.api_url, headers=headers, params=params)

        if response.status_code == 200:
            data = response.json()

            if 'value' in data and isinstance(data['value'], list):
                rows = data['value'][:100]
                headers = list(rows[0].keys()) if rows else []
                return JsonResponse({
                    'headers': headers,
                    'rows': rows
                })

            return JsonResponse({'headers': [], 'rows': []})

        return JsonResponse({'error': f'HTTP {response.status_code}: {response.text}'}, status=response.status_code)

    except SAPApi.DoesNotExist:
        return JsonResponse({'error': 'SAP API config not found'}, status=404)

    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)
    

@csrf_exempt
def sql_query_data(request, id):
    try:
        query = DynamicQuery.objects.get(id=id)

        with connection.cursor() as cursor:
            cursor.execute(query.query)
            column_names = [col[0] for col in cursor.description] if cursor.description else []
            rows = cursor.fetchall()

            exclude_fields = {'id', 'ID', 'loader_instance'}

            filtered_headers = [col for col in column_names if col not in exclude_fields]
            rows_dicts = [
                {col: val for col, val in zip(column_names, row) if col not in exclude_fields}
                for row in rows[:100]
            ]

        return JsonResponse({
            'headers': filtered_headers,
            'rows': rows_dicts
        })

    except DynamicQuery.DoesNotExist:
        return JsonResponse({'error': 'DynamicQuery config not found'}, status=404)

    except Exception as e:
        return JsonResponse({'error': str(e)}, status=500)    

@shared_task
def local_migrations(result):
    try:
        print("Starting migrations...")
        subprocess.run('python manage.py makemigrations --noinput', shell=True, check=True)
        print("Makemigrations completed.")
        subprocess.run('python manage.py migrate --noinput', shell=True, check=True)
        print("Migrate completed.")
        result['status'] = 'completed'
        return result
    except subprocess.CalledProcessError as e:
        result['status'] = 'error'
        result['message'] = str(e)
        raise

@shared_task
def create_views_urls_templates(result):
    try:
        if result['status'] == 'completed':
            request_data = result['request_data']
            model_name = f"tables.{result['model_name']}"
            view_function(request_data, model_name)
            url_function(request_data)
            template_function(request_data, model_name)
            print(f"Views, URLs, and templates for {model_name} created successfully!")
        else:
            print("Failed to generate views, URLs, and templates due to model generation error.")
        return result
    except Exception as e:
        result['status'] = 'error'
        result['message'] = str(e)


@shared_task
def create_model_and_migrate(request_data):
    model_name = request_data.get('model_name')

    columns = {}
    date_fields_to_convert = []
    integer_fields = []
    float_fields = []
    encrypted_fields = []
    unix_dates = []
    ad_unix_dates = []
    has_auto_field = False

    for key, value in request_data.items():
        if key.startswith('columns_') and not key.endswith('_type'):
            column_name = key.split('_', 1)[1]
            sanitized_column_name = column_name.strip().replace(' ', '_')

            if column_name.lower() == 'count' or column_name.lower() == 'id':
                sanitized_column_name = f"{sanitized_column_name}_original"

            column_type = request_data.get(f'columns_{column_name}_type', 'TextField')
            db_column = request_data.get(f'db_columns_{column_name}', sanitized_column_name)
            encryption_column = request_data.get(f'encryption_{column_name}') == 'on'
            
            db_column = db_column.strip()
            db_column = re.sub(r'[!@#$%^&*()+\-=[\]{}|;:\'",.<>/?]', '', db_column)
            db_column = re.sub(r'__+', '_', db_column)
            db_column = db_column.rstrip('_')
            db_column = re.sub(r'^\d+', '', db_column)

            if not db_column:
                db_column = sanitized_column_name

            # if column_name.lower() == 'id' and not has_auto_field:
            #     columns[sanitized_column_name] = {
            #         'type': 'AutoField',
            #         'db_column': db_column,
            #         'primary_key': True
            #     }
            #     has_auto_field = True
            # else:
            #     columns[sanitized_column_name] = {'type': column_type, 'db_column': db_column}
            
            columns[sanitized_column_name] = {'type': column_type, 'db_column': db_column}

            if column_type == 'DateField':
                date_fields_to_convert.append(sanitized_column_name)
            elif column_type == 'IntegerField':
                integer_fields.append(sanitized_column_name)
            elif column_type == 'FloatField':
                float_fields.append(sanitized_column_name)
            elif column_type == 'UnixDates':
                date_fields_to_convert.append(sanitized_column_name)
                unix_dates.append(sanitized_column_name)
            elif column_type == 'ADUnixDates':
                date_fields_to_convert.append(sanitized_column_name)
                ad_unix_dates.append(sanitized_column_name)
            

            if encryption_column:
                encrypted_fields.append(sanitized_column_name)

    model_code = f"\n\nclass {model_name}(models.Model):\n"
    if not has_auto_field:
        model_code += "    ID = models.AutoField(primary_key=True)\n"

    for column, column_info in columns.items():
        column_type = column_info['type']
        db_column = column_info['db_column']

        if column_type == 'AutoField':
            model_code += f"    {column} = models.AutoField(primary_key=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        elif column_type == 'CharField':
            model_code += f"    {column} = models.CharField(max_length=255, null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        elif column_type == 'IntegerField':
            model_code += f"    {column} = models.IntegerField(null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        elif column_type == 'FloatField':
            model_code += f"    {column} = models.FloatField(null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        elif column_type == 'DateField':
            model_code += f"    {column} = models.BigIntegerField(null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        elif column_type == 'BigIntegerField':
            model_code += f"    {column} = models.BigIntegerField(null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        elif column_type == 'UnixDates':
            model_code += f"    {column} = models.BigIntegerField(null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        elif column_type == 'ADUnixDates':
            model_code += f"    {column} = models.BigIntegerField(null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"
        else:
            model_code += f"    {column} = models.TextField(null=True, blank=True, verbose_name='{db_column}', db_column='{db_column}')\n"

    model_code += "    loader_instance = models.IntegerField(null=True, blank=True)\n"
    model_code += "    json_data = models.JSONField(null=True, blank=True)\n"
    if is_pgvector_enabled():
        model_code += "    hash_data = VectorField(dimensions=384, null=True, blank=True)\n"
    else:
        model_code += "    hash_data = models.TextField(null=True, blank=True)\n"

    model_code += f"\n    date_fields_to_convert = {date_fields_to_convert}\n"
    model_code += f"    integer_fields = {integer_fields}\n"
    model_code += f"    float_fields = {float_fields}\n"
    model_code += f"    encrypted_fields = {encrypted_fields}\n"
    model_code += f"    unix_dates = {unix_dates}\n"
    model_code += f"    ad_unix_dates = {ad_unix_dates}\n"

    models_file_path = getattr(settings, 'MODELS_FILE_PATH')
    with open(models_file_path, 'a') as f:
        f.write(model_code)

    update_model_choices_file(model_name)

    task_chain = chain(
        local_migrations.s({'status': 'pending', 'request_data': request_data, 'model_name': model_name}),
        create_views_urls_templates.s()
    )
    result = task_chain.apply_async()
    task_id = result.id

    return {
        'status': 'pending',
        'task_id': task_id,
        'message': 'Model creation in progress. Please wait...'
    }


def model_builder(request):
    if request.method == 'POST':
        model_name = request.POST.get('model_name')
        if model_exists(model_name):
            return JsonResponse({
                'status': 'error',
                'message': f"Model '{model_name}' already exists."
            })
    
        if table_exists(f'tables_{model_name.lower()}'):
            return JsonResponse({
                'status': 'error',
                'message': f'Table for model "{model_name}" already exists in the database.'
            })
    
        post_data = request.POST.dict()
        for key in ['ro_fields', 'precol_fields', 'compulsory_fields']:
            post_data[key] = request.POST.getlist(key)

        result = create_model_and_migrate.apply_async(args=[post_data])
        task_id = result.id

        fav_name = request.POST.get('fav_name')
        view_name = fav_name.replace(' ', '_').lower()
        view_name = re.sub(r'[^a-zA-Z0-9_]', '', view_name)

        sidebar_group = get_object_or_404(Group, name=request.POST.get('group_name'))
        sidebar_parent = get_object_or_404(Sidebar, id=request.POST.get('parent_name'))
        Sidebar.objects.create(
            group=sidebar_group,
            parent=sidebar_parent,
            name=fav_name,
            segment=view_name,
            url_name=view_name
        )

        return JsonResponse({
            'status': 'pending',
            'task_id': task_id,
            'view_name': view_name,
            'message': 'Model creation in progress. Please wait...'
        })

    loader_model_apps = settings.LOADER_MODEL_APPS
    model_names = {}
    for app_name in loader_model_apps:
        app_config = apps.get_app_config(app_name)
        models = [model.__name__ for model in app_config.get_models()]
        model_names[app_name] = models

    sidebars = Sidebar.objects.filter(is_active=True, parent__isnull=True).values('id', 'name')
    groups = Group.objects.all()

    context = {
        'model_names': model_names,
        'sidebars': sidebars,
        'groups': groups,
        'sap_apis': SAPApi.objects.all(),
        'segment': 'model_builder',
        'parent' : 'home'
    }
    return render(request, 'pages/model_builder.html', context)
